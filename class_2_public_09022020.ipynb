{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class 2: Numpy, SciPy, Pandas, Statmodes and advanced visualization\n",
    "\n",
    "\n",
    "## What will we learn in this class?\n",
    "\n",
    "* Numpy\n",
    "* Pandas\n",
    "* Matplotlib\n",
    "* Seaborn\n",
    "* Sci-py\n",
    "\n",
    "Sources:\n",
    "\n",
    "    Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython  by Wes McKinney\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tilulo principal\n",
    "## subitutulo\n",
    "\n",
    "$$\n",
    "formula=\\frac{1}{2}\\delta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numpy\n",
    "\n",
    "NumPy, short for Numerical Python, is the foundational package for scientific computing in Python. \n",
    "on top of NumPy. It provides, among other things:\n",
    "• A fast and efficient multidimensional array object ndarray\n",
    "• Functions for performing element-wise computations with arrays or mathematical\n",
    "operations between arrays\n",
    "• Tools for reading and writing array-based data sets to disk\n",
    "• Linear algebra operations, Fourier transform, and random number generation\n",
    "• Tools for integrating connecting C, C++, and Fortran code to Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding examples\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.00109818,  0.07044009, -1.02516603],\n",
       "       [ 0.63438484, -0.58434263, -0.45864191]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=np.random.randn(2, 3)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.01098182,   0.70440091, -10.25166034],\n",
       "       [  6.34384838,  -5.84342626,  -4.58641907]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#broadcasting\n",
    "data * 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.00219636,  0.14088018, -2.05033207],\n",
       "       [ 1.26876968, -1.16868525, -0.91728381]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data + data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.00219636, -0.93065809, -2.02626422],\n",
       "       [ 1.26876968,  0.05004221,  0.17574293]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data+data[:,0].reshape(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00219757, 0.00496181, 1.0509654 ],\n",
       "       [0.40244412, 0.3414563 , 0.2103524 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data*data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6. , 7.5, 8. , 0. , 1. ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#crear arreglos\n",
    "data1 = [6, 7.5, 8, 0, 1]\n",
    "arr1 = np.array(data1)\n",
    "arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.00219636, -0.93065809, -2.02626422],\n",
       "       [ 1.26876968,  0.05004221,  0.17574293]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data+data[:,0].reshape(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:,0].reshape(2,1).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4.],\n",
       "       [5., 6., 7., 8.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = [[1, 2.0, 3, 4], [5, 6, 7, 8]]\n",
    "arr2 = np.array(data2)\n",
    "arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(10)\n",
    "np.zeros((3,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1., 2., 3.], [4., 5., 6.]])\n",
    "arr\n",
    "arr * arr\n",
    "arr - arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.5       , 0.33333333],\n",
       "       [0.25      , 0.2       , 0.16666667]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 4.],\n",
       "       [2., 5.],\n",
       "       [3., 6.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.41421356, 1.73205081],\n",
       "       [2.        , 2.23606798, 2.44948974]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  4.,  1.],\n",
       "       [ 7.,  2., 12.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#operacion logica\n",
    "arr2 = np.array([[0., 4., 1.], [7., 2., 12.]])\n",
    "arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True, False],\n",
       "       [ True, False,  True]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2 > arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [2. 1.]]\n",
      "[[5. 4.]\n",
      " [4. 5.]]\n"
     ]
    }
   ],
   "source": [
    "m1=np.array([[1., 2.], [2., 1]])\n",
    "print(m1)\n",
    "print(np.matmul(m1,m1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 4.],\n",
       "       [4., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1*m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#indexing and slicing\n",
    "arr = np.arange(10)*10\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50, 60, 70])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[5:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 10, 20, 30, 40, 12, 12, 12, 80, 90])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[5:8] = 12\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'], dtype='<U4')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.77998565,  0.72839899,  0.91219484,  0.14724693],\n",
       "       [-0.48710535,  0.08539872,  0.12700474, -0.6886856 ],\n",
       "       [ 0.68855954, -1.63779496, -0.12386654, -0.04898135],\n",
       "       [ 0.10681277, -0.77996196, -0.39112614,  0.3190114 ],\n",
       "       [ 0.53554462, -0.84737767, -1.19891363, -0.75775915],\n",
       "       [-1.4771606 ,  1.57182604,  1.62877025, -0.60554916],\n",
       "       [ 0.98658569,  1.45244582, -1.03589323,  1.55180805]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.random.randn(7, 4)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False,  True, False, False, False])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names == 'Bob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.77998565,  0.72839899,  0.91219484,  0.14724693],\n",
       "       [ 0.10681277, -0.77996196, -0.39112614,  0.3190114 ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[names == 'Bob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.91219484,  0.14724693],\n",
       "       [-0.39112614,  0.3190114 ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[names == 'Bob', 2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14724693, 0.3190114 ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[names == 'Bob', 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14724693, 0.3190114 ])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[names == 'Bob'][:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As Data Scientist we will be using numpy particularly in two instances\n",
    "\n",
    "1. Speeding pandas \n",
    "2. Creating tensors to feed our machine learning algorithms\n",
    "\n",
    "<img src=\"static/img/tensor.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets see how we can create a multidimensional Tensor. For this exercise we will read csv files withouth headers and indices from 11 different assets: Microsoft, Netflix, Intel, Google, Nvidia, Amazon, Tesla,Apple, Facebook, Paypal and S&P oil and gas sector. Each file contains a time serie with 1minute bars from each stock or etf. \n",
    "\n",
    "For this exercise we will read each time serie individually, save it in a dictionary and the  create a 5x5x11 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAPL_Stocks__NASDAQnc.csv',\n",
       " 'AMZN_Stocks__NASDAQnc.csv',\n",
       " 'FB_Stocks__NASDAQnc.csv',\n",
       " 'GOOG_Stocks__NASDAQnc.csv',\n",
       " 'INTC_Stocks__NASDAQnc.csv',\n",
       " 'MSFT_Stocks__NASDAQnc.csv',\n",
       " 'NFLX_Stocks__NASDAQnc.csv',\n",
       " 'NVDA_Stocks__NASDAQnc.csv',\n",
       " 'PYPL_Stocks__NASDAQnc.csv',\n",
       " 'QQQ_ETF__NASDAQnc.csv',\n",
       " 'TSLA_Stocks__NASDAQnc.csv',\n",
       " 'XLE_ETF__AMEXnc.csv']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_names=[f for f in os.listdir(\"data/\") if \"nc.csv\" in f]\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAPL_Stocks__NASDAQ.csv',\n",
       " 'AAPL_Stocks__NASDAQnc.csv',\n",
       " 'AMZN_Stocks__NASDAQ.csv',\n",
       " 'AMZN_Stocks__NASDAQnc.csv',\n",
       " 'FB_Stocks__NASDAQ.csv',\n",
       " 'FB_Stocks__NASDAQnc.csv',\n",
       " 'GOOG_Stocks__NASDAQ.csv',\n",
       " 'GOOG_Stocks__NASDAQnc.csv',\n",
       " 'INTC_Stocks__NASDAQ.csv',\n",
       " 'INTC_Stocks__NASDAQnc.csv',\n",
       " 'MSFT_Stocks__NASDAQ.csv',\n",
       " 'MSFT_Stocks__NASDAQnc.csv',\n",
       " 'NFLX_Stocks__NASDAQ.csv',\n",
       " 'NFLX_Stocks__NASDAQnc.csv',\n",
       " 'NVDA_Stocks__NASDAQ.csv',\n",
       " 'NVDA_Stocks__NASDAQnc.csv',\n",
       " 'PYPL_Stocks__NASDAQ.csv',\n",
       " 'PYPL_Stocks__NASDAQnc.csv',\n",
       " 'QQQ_ETF__NASDAQ.csv',\n",
       " 'QQQ_ETF__NASDAQnc.csv',\n",
       " 'TSLA_Stocks__NASDAQ.csv',\n",
       " 'TSLA_Stocks__NASDAQnc.csv',\n",
       " 'XLE_ETF__AMEX.csv',\n",
       " 'XLE_ETF__AMEXnc.csv']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names=[f for f in os.listdir(\"data/\") if \"nc.csv\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1029493, 5)\n",
      "(1027391, 5)\n",
      "(795631, 5)\n",
      "(599181, 5)\n",
      "(1029481, 5)\n",
      "(1029534, 5)\n",
      "(1025770, 5)\n",
      "(1038832, 5)\n",
      "(486964, 5)\n",
      "(1040095, 5)\n",
      "(959423, 5)\n",
      "(1029420, 5)\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "arrays={}\n",
    "for file in file_names:\n",
    "    arrays[file]=genfromtxt(\"data/\"+file, delimiter=',')\n",
    "    print(arrays[file].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.892858e+00, 7.961429e+00, 7.885715e+00, 7.892858e+00,\n",
       "        1.451520e+05],\n",
       "       [7.902857e+00, 7.928572e+00, 7.894286e+00, 7.911429e+00,\n",
       "        8.267000e+04],\n",
       "       [7.917143e+00, 7.921429e+00, 7.898572e+00, 7.898572e+00,\n",
       "        1.046000e+05],\n",
       "       [7.898572e+00, 7.911429e+00, 7.894286e+00, 7.898572e+00,\n",
       "        4.759900e+04],\n",
       "       [7.904286e+00, 7.927144e+00, 7.892858e+00, 7.911429e+00,\n",
       "        9.499600e+04]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "arrays[\"NFLX_Stocks__NASDAQnc.csv\"][:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tensor\n",
    "\n",
    "prices_tensor=np.zeros((5,5,12))\n",
    "counter=0\n",
    "for key,value in arrays.items():\n",
    "    prices_tensor[:,:,counter]=value[:5,:]\n",
    "    counter=counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "frutas\n",
      "====\n",
      "['mango', 'fresa']\n",
      "1\n",
      "verduras\n",
      "====\n",
      "['cebollas', 'pepinos']\n"
     ]
    }
   ],
   "source": [
    "temp_dict={\"frutas\":[\"mango\",\"fresa\"],\"verduras\":[\"cebollas\",\"pepinos\"]}\n",
    "counter=0\n",
    "for key,value in temp_dict.items():\n",
    "    print(counter)\n",
    "    print(key)\n",
    "    print(\"====\")\n",
    "    print(value)\n",
    "    counter=counter+1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 12)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_tensor[:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pandas\n",
    "pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
    "built on top of the Python programming language. \n",
    "\n",
    "When working with tabular data, such as data stored in spreadsheets or databases, pandas is the right tool for you. pandas will help you to explore, clean and process your data. In pandas, a data table is called a DataFrame.\n",
    "\n",
    "lets check this: https://pandas.pydata.org/docs/getting_started/intro_tutorials/01_table_oriented.html#min-tut-01-tableoriented\n",
    "\n",
    "\n",
    "Lets keep working with our data and read the csv files\n",
    "\n",
    "sources:\n",
    "https://pandas.pydata.org/\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL_Stocks__NASDAQ.csv', 'AMZN_Stocks__NASDAQ.csv', 'FB_Stocks__NASDAQ.csv', 'GOOG_Stocks__NASDAQ.csv', 'INTC_Stocks__NASDAQ.csv', 'MSFT_Stocks__NASDAQ.csv', 'NFLX_Stocks__NASDAQ.csv', 'NVDA_Stocks__NASDAQ.csv', 'PYPL_Stocks__NASDAQ.csv', 'QQQ_ETF__NASDAQ.csv', 'TSLA_Stocks__NASDAQ.csv', 'XLE_ETF__AMEX.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_file_names=[f for f in os.listdir(\"data/\") if \"nc.csv\" not in f]\n",
    "print(pandas_file_names)\n",
    "fin_data_df={}\n",
    "for file in pandas_file_names:\n",
    "    fin_data_df[file]=pd.read_csv(\"data/\"+file, index_col=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-05-18 15:30:00+00:00</th>\n",
       "      <td>42.050</td>\n",
       "      <td>45.00</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>42.230</td>\n",
       "      <td>83177720.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-18 15:31:00+00:00</th>\n",
       "      <td>42.270</td>\n",
       "      <td>43.00</td>\n",
       "      <td>41.8300</td>\n",
       "      <td>42.010</td>\n",
       "      <td>5523794.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-18 15:32:00+00:00</th>\n",
       "      <td>42.010</td>\n",
       "      <td>42.99</td>\n",
       "      <td>40.0000</td>\n",
       "      <td>41.400</td>\n",
       "      <td>6662136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-18 15:33:00+00:00</th>\n",
       "      <td>41.270</td>\n",
       "      <td>42.66</td>\n",
       "      <td>39.4400</td>\n",
       "      <td>40.940</td>\n",
       "      <td>5206776.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-18 15:34:00+00:00</th>\n",
       "      <td>40.900</td>\n",
       "      <td>41.89</td>\n",
       "      <td>38.9100</td>\n",
       "      <td>40.250</td>\n",
       "      <td>2921595.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-07 19:56:00+00:00</th>\n",
       "      <td>240.720</td>\n",
       "      <td>241.07</td>\n",
       "      <td>240.6283</td>\n",
       "      <td>240.830</td>\n",
       "      <td>129679.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-07 19:57:00+00:00</th>\n",
       "      <td>240.845</td>\n",
       "      <td>241.35</td>\n",
       "      <td>240.8450</td>\n",
       "      <td>241.220</td>\n",
       "      <td>123966.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-07 19:58:00+00:00</th>\n",
       "      <td>241.210</td>\n",
       "      <td>241.25</td>\n",
       "      <td>241.0600</td>\n",
       "      <td>241.105</td>\n",
       "      <td>109628.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-07 19:59:00+00:00</th>\n",
       "      <td>241.080</td>\n",
       "      <td>241.12</td>\n",
       "      <td>240.8300</td>\n",
       "      <td>240.860</td>\n",
       "      <td>327943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-07 20:00:00+00:00</th>\n",
       "      <td>240.860</td>\n",
       "      <td>240.86</td>\n",
       "      <td>240.8600</td>\n",
       "      <td>240.860</td>\n",
       "      <td>1121390.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>795631 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              open    high       low    close      volume\n",
       "time                                                                     \n",
       "2012-05-18 15:30:00+00:00   42.050   45.00   42.0000   42.230  83177720.0\n",
       "2012-05-18 15:31:00+00:00   42.270   43.00   41.8300   42.010   5523794.0\n",
       "2012-05-18 15:32:00+00:00   42.010   42.99   40.0000   41.400   6662136.0\n",
       "2012-05-18 15:33:00+00:00   41.270   42.66   39.4400   40.940   5206776.0\n",
       "2012-05-18 15:34:00+00:00   40.900   41.89   38.9100   40.250   2921595.0\n",
       "...                            ...     ...       ...      ...         ...\n",
       "2020-07-07 19:56:00+00:00  240.720  241.07  240.6283  240.830    129679.0\n",
       "2020-07-07 19:57:00+00:00  240.845  241.35  240.8450  241.220    123966.0\n",
       "2020-07-07 19:58:00+00:00  241.210  241.25  241.0600  241.105    109628.0\n",
       "2020-07-07 19:59:00+00:00  241.080  241.12  240.8300  240.860    327943.0\n",
       "2020-07-07 20:00:00+00:00  240.860  240.86  240.8600  240.860   1121390.0\n",
       "\n",
       "[795631 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data is structured and easier to read\n",
    "fb_data_df=fin_data_df[\"FB_Stocks__NASDAQ.csv\"]\n",
    "fb_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-05-18 15:30:00+00:00</th>\n",
       "      <td>42.05</td>\n",
       "      <td>45.00</td>\n",
       "      <td>42.00</td>\n",
       "      <td>42.23</td>\n",
       "      <td>83177720.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-18 15:31:00+00:00</th>\n",
       "      <td>42.27</td>\n",
       "      <td>43.00</td>\n",
       "      <td>41.83</td>\n",
       "      <td>42.01</td>\n",
       "      <td>5523794.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-18 15:32:00+00:00</th>\n",
       "      <td>42.01</td>\n",
       "      <td>42.99</td>\n",
       "      <td>40.00</td>\n",
       "      <td>41.40</td>\n",
       "      <td>6662136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-18 15:33:00+00:00</th>\n",
       "      <td>41.27</td>\n",
       "      <td>42.66</td>\n",
       "      <td>39.44</td>\n",
       "      <td>40.94</td>\n",
       "      <td>5206776.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-18 15:34:00+00:00</th>\n",
       "      <td>40.90</td>\n",
       "      <td>41.89</td>\n",
       "      <td>38.91</td>\n",
       "      <td>40.25</td>\n",
       "      <td>2921595.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            open   high    low  close      volume\n",
       "time                                                             \n",
       "2012-05-18 15:30:00+00:00  42.05  45.00  42.00  42.23  83177720.0\n",
       "2012-05-18 15:31:00+00:00  42.27  43.00  41.83  42.01   5523794.0\n",
       "2012-05-18 15:32:00+00:00  42.01  42.99  40.00  41.40   6662136.0\n",
       "2012-05-18 15:33:00+00:00  41.27  42.66  39.44  40.94   5206776.0\n",
       "2012-05-18 15:34:00+00:00  40.90  41.89  38.91  40.25   2921595.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top rows\n",
    "fb_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-07-07 19:56:00+00:00</th>\n",
       "      <td>240.720</td>\n",
       "      <td>241.07</td>\n",
       "      <td>240.6283</td>\n",
       "      <td>240.830</td>\n",
       "      <td>129679.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-07 19:57:00+00:00</th>\n",
       "      <td>240.845</td>\n",
       "      <td>241.35</td>\n",
       "      <td>240.8450</td>\n",
       "      <td>241.220</td>\n",
       "      <td>123966.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-07 19:58:00+00:00</th>\n",
       "      <td>241.210</td>\n",
       "      <td>241.25</td>\n",
       "      <td>241.0600</td>\n",
       "      <td>241.105</td>\n",
       "      <td>109628.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-07 19:59:00+00:00</th>\n",
       "      <td>241.080</td>\n",
       "      <td>241.12</td>\n",
       "      <td>240.8300</td>\n",
       "      <td>240.860</td>\n",
       "      <td>327943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-07 20:00:00+00:00</th>\n",
       "      <td>240.860</td>\n",
       "      <td>240.86</td>\n",
       "      <td>240.8600</td>\n",
       "      <td>240.860</td>\n",
       "      <td>1121390.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              open    high       low    close     volume\n",
       "time                                                                    \n",
       "2020-07-07 19:56:00+00:00  240.720  241.07  240.6283  240.830   129679.0\n",
       "2020-07-07 19:57:00+00:00  240.845  241.35  240.8450  241.220   123966.0\n",
       "2020-07-07 19:58:00+00:00  241.210  241.25  241.0600  241.105   109628.0\n",
       "2020-07-07 19:59:00+00:00  241.080  241.12  240.8300  240.860   327943.0\n",
       "2020-07-07 20:00:00+00:00  240.860  240.86  240.8600  240.860  1121390.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bottom rows\n",
    "fb_data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-05-18 15:31:00+00:00</th>\n",
       "      <td>42.27</td>\n",
       "      <td>43.00</td>\n",
       "      <td>41.83</td>\n",
       "      <td>42.01</td>\n",
       "      <td>5523794.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-18 15:32:00+00:00</th>\n",
       "      <td>42.01</td>\n",
       "      <td>42.99</td>\n",
       "      <td>40.00</td>\n",
       "      <td>41.40</td>\n",
       "      <td>6662136.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            open   high    low  close     volume\n",
       "time                                                            \n",
       "2012-05-18 15:31:00+00:00  42.27  43.00  41.83  42.01  5523794.0\n",
       "2012-05-18 15:32:00+00:00  42.01  42.99  40.00  41.40  6662136.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#any rows\n",
    "#by number use iloc\n",
    "fb_data_df.iloc[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#by index value use loc\n",
    "random_index=np.random.randint(0,fb_data_df.shape[1],2)\n",
    "random_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2012-05-18 15:31:00+00:00', '2012-05-18 15:31:00+00:00']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "random_dates=fb_data_df.index[random_index.tolist()].values.tolist()\n",
    "random_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open         241.210\n",
       "high         241.250\n",
       "low          241.060\n",
       "close        241.105\n",
       "volume    109628.000\n",
       "Name: 2020-07-07 19:58:00+00:00, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_data_df.loc[\"2020-07-07 19:58:00+00:00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2012-05-18 15:30:00+00:00', '2012-05-18 15:31:00+00:00',\n",
       "               '2012-05-18 15:32:00+00:00', '2012-05-18 15:33:00+00:00',\n",
       "               '2012-05-18 15:34:00+00:00', '2012-05-18 15:35:00+00:00',\n",
       "               '2012-05-18 15:36:00+00:00', '2012-05-18 15:37:00+00:00',\n",
       "               '2012-05-18 15:38:00+00:00', '2012-05-18 15:39:00+00:00',\n",
       "               ...\n",
       "               '2020-07-07 19:51:00+00:00', '2020-07-07 19:52:00+00:00',\n",
       "               '2020-07-07 19:53:00+00:00', '2020-07-07 19:54:00+00:00',\n",
       "               '2020-07-07 19:55:00+00:00', '2020-07-07 19:56:00+00:00',\n",
       "               '2020-07-07 19:57:00+00:00', '2020-07-07 19:58:00+00:00',\n",
       "               '2020-07-07 19:59:00+00:00', '2020-07-07 20:00:00+00:00'],\n",
       "              dtype='datetime64[ns, UTC]', name='time', length=795631, freq=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indice_2=pd.to_datetime(fb_data_df.index)\n",
    "indice_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['open', 'high', 'low', 'close', 'volume'], dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also call the columns\n",
    "fb_data_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Complex functions with apply\n",
    "\n",
    "source:\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B\n",
       "0  4  9\n",
       "1  4  9\n",
       "2  4  9"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B\n",
       "0  2.0  3.0\n",
       "1  2.0  3.0\n",
       "2  2.0  3.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    13\n",
       "1    13\n",
       "2    13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(np.sum, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    12\n",
       "B    27\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(np.sum, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [1, 2]\n",
       "1    [1, 2]\n",
       "2    [1, 2]\n",
       "dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(lambda x: [1, 2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pandas Data types\n",
    "\n",
    "before going forward lets make a break to discuss the data types of pandas dataframes. In many cases the types will be naturally infered. However, when it is not the case we need to specify them, specially if we want to  use the full pandas functionality. \n",
    "\n",
    "Lets see what happens if we want to get all the data after 2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-21 02:19:04.771263\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "break_date=datetime.datetime.utcnow()-datetime.timedelta(days=500)\n",
    "print(break_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(break_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open      float64\n",
       "high      float64\n",
       "low       float64\n",
       "close     float64\n",
       "volume    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_data_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'str' and 'datetime.datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-a4fb55bd8d84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfb_data_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfb_data_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mbreak_date\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\venv\\p38_class_venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mcmp_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;31m# don't pass MultiIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\p38_class_venv\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\ops.pyx\u001b[0m in \u001b[0;36mpandas._libs.ops.scalar_compare\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'str' and 'datetime.datetime'"
     ]
    }
   ],
   "source": [
    "fb_data_df[fb_data_df.index>break_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ups , error our index is not a datetime index, so we need to convert it. lets do this\n",
    "fb_data_df.index=pd.to_datetime(fb_data_df.index)\n",
    "fb_data_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_data_df[fb_data_df.index>break_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "\n",
    "break_date.replace(tzinfo=pytz.utc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_data_df[fb_data_df.index>break_date.replace(tzinfo=pytz.utc)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " Pandas can work very nicely with time series and time indices. While we are using utc dates in the index and the target date, this is not obligatory. We could have requested a slice using any time zone and pandas will automatically convert. Very useful when trading across time zones. \n",
    " \n",
    " ### Group by: split-apply-combine\n",
    " \n",
    " \n",
    " source:\n",
    " \n",
    " https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see an example. lets group our data by  day of trade\n",
    "fb_data_df[\"day_of_trade\"]=[i.date() for i in fb_data_df.index]\n",
    "\n",
    "fb_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_data_df.groupby([\"day_of_trade\",\"time\"]).last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can create operations on groups using transform. lets suppose we want to assign the last trade\n",
    "#price of each trade day to each overvation\n",
    "\n",
    "fb_data_df.iloc[:1000].groupby([\"day_of_trade\"])[\"close\"].transform(lambda x: max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check if it is right\n",
    "unique_days=fb_data_df.day_of_trade.unique()\n",
    "fb_data_df[fb_data_df.day_of_trade<=unique_days[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_data_df.iloc[0:1000][fb_data_df.iloc[0:1000].day_of_trade>=fb_data_df.iloc[0:1000].day_of_trade.values[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Resampling\n",
    "\n",
    "Instead of grouping we can ask pandas directly to resample our data. \n",
    "\n",
    "source:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.resample.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_data_df.resample(\"5min\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Merging\n",
    "\n",
    "Very often when we work with pandas we dont want to use tensors but column arrangements as features. We can achieve this in many ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df=fin_data_df[\"AMZN_Stocks__NASDAQ.csv\"]\n",
    "amazon_df.index=pd.to_datetime(amazon_df.index)\n",
    "print(amazon_df.shape)\n",
    "print(fb_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df.columns=[col+\"_AMZN\" for col in amazon_df]\n",
    "amazon_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([fb_data_df,amazon_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([amazon_df,fb_data_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also use a more advanced merge  inspired on sql joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df.merge(fb_data_df,left_index=True,right_index=True,how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df.merge(fb_data_df,left_index=True,right_index=True,how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df.merge(fb_data_df,left_index=True,right_index=True,how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=amazon_df.merge(fb_data_df,left_index=True,right_index=True,how=\"inner\")[[\"close\",\"close_AMZN\"]].resample(\"1d\").last().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matplolib and Seaborn\n",
    "\n",
    "Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
    "\n",
    "Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "\n",
    "Matplotlib is the standard pandas ploting library, lets see a few visualizations\n",
    "\n",
    "sources:\n",
    "\n",
    "https://matplotlib.org/\n",
    "\n",
    "https://seaborn.pydata.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn_merg=amazon_df.merge(fb_data_df,left_index=True,right_index=True,how=\"left\")\n",
    "missing_values=amzn_merg[amzn_merg.isnull().any(axis=1)]\n",
    "missing_values\n",
    "\n",
    "close_monthly=amzn_merg[[col for col in amzn_merg.columns if \"close\" in col]].resample(\"1m\").last().dropna()\n",
    "close_monthly.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2,figsize=(10,5))\n",
    "axs[0].plot(close_monthly.close)\n",
    "axs[0].title.set_text('Facebook ')\n",
    "axs[0].set_ylabel(\"price\")\n",
    "\n",
    "axs[1].plot(np.log(close_monthly[\"close_AMZN\"]))\n",
    "axs[1].title.set_text('Amazon ')\n",
    "axs[1].set_ylabel(\"log price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g=sns.pairplot(np.log(close_monthly).diff().dropna())\n",
    "g.fig.set_size_inches(7,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scipy\n",
    "\n",
    "SciPy (pronounced “Sigh Pie”) is open-source software for mathematics, science, and engineering.\n",
    "\n",
    "Most of the libraries that you will use,  provide you with optimzed solutions. For the cases that when you need a simple multi porpuse optimizer you can use scipy.\n",
    "\n",
    "The scipy.optimize package provides several commonly used optimization algorithms. A detailed listing is available: scipy.optimize (can also be found by help(scipy.optimize)).\n",
    "\n",
    "The module contains:\n",
    "\n",
    "    Unconstrained and constrained minimization of multivariate scalar functions (minimize) using a variety of algorithms (e.g. BFGS, Nelder-Mead simplex, Newton Conjugate Gradient, COBYLA or SLSQP)\n",
    "    Global (brute-force) optimization routines (e.g., anneal, basinhopping)\n",
    "    Least-squares minimization (leastsq) and curve fitting (curve_fit) algorithms\n",
    "    Scalar univariate functions minimizers (minimize_scalar) and root finders (newton)\n",
    "    Multivariate equation system solvers (root) using a variety of algorithms (e.g. hybrid Powell, Levenberg-Marquardt or large-scale methods such as Newton-Krylov).\n",
    "\n",
    "\n",
    "\n",
    "source:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Good, The Bad and The Ugly\n",
    "\n",
    "The Good: Pandas can help us working with tabular multidimensional data in a simpler and declarative way. \n",
    "\n",
    "The Bad: When working with big data ( millions of rows and several columns) We cant use dataframes as they are  loaded in memory. This implies that pandas methods may not run or raise exception when trying to combine/aggregate big data. \n",
    "\n",
    "The Ugly: Pandas can be very slow if using the wrong method (and in general is slower than Numpy) if what you are looking is speed then you may need to work with Numpy arrays or even more you will need to use Cython. \n",
    "\n",
    "Lets look at an example:\n",
    "\n",
    "From  Chapter 5 of the Advances in Financial Machine Learning (AFML) by Dr. Marcos Lopez de Prado therein he discusses fractionally differencing the time series (as opposed to integer differencing). A fractionally differentiated series is stationary but also has high correlation with the original series. Since a fractionally differenced series retains the memory of the original series (as indicated by the high correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Why Fractional Differentiation?\n",
    "\n",
    "Inferential analysis of data comprises of using a sample of data to describe the characteristics such as mean and standard deviation of a feature of a population. Consider studying heights of men and women in North America or stock prices. For such an analysis and inference to be accurate, it is necessary that the underlying data generation process to remain constant. In the context of finance, the mean return and variance of those returns should be time-invariant (or not change with time). If the underlying process changes as a result of shift in regime, it would be hard to predict expected risk and return of that stock for a future date. A similar requirement exists in the case of supervised machine learning (SML). SML is a process of learning a function that maps an input to an output based on known input-output examples. In this learning process, each example is a pair consisting of an input object (often a vector or features) and an output (or a signal). The supervised learning algorithm analyzes the training examples and infers a transformation function that can be used to map new (unseen) inputs. If the data (features, in the case of SML) are not “stationary” (in other words, their underlying data generation process changes its characteristics) then the machine learning algorithm would not be able to correctly infer the label of the new observation. Therefore, stationarity becomes a necessary condition for inferential analysis and supervised machine learning. But there is a problem here – even though making a series stationary makes inference analysis and SML easier, the series loses its memory (it probably had a trend and that trend is stripped away in the process of integer differencing). This memory is helpful in predicting where will the asset price series be next point in time. This leads to a challenge – how can one make the time series stationary while retaining its predictive power (or memory).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "sources:\n",
    "\n",
    "\n",
    "https://cython.org/\n",
    "\n",
    "https://engineering.upside.com/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One of the easiest ways to make your code in pandas very slow is to loop over loc searchers.In terms of speed this is my recomendation\n",
    "\n",
    "1. Use iloc vs loc when possible\n",
    "2. If you can do your computations with iloc , then you can do also using numpy\n",
    "3. When slicing a data frame, use function ```isin()``` for example\n",
    "```\n",
    "data[data.index.isin([date])] \n",
    "#rather than\n",
    "data[data.index==date]\n",
    "\n",
    "```\n",
    "4. SQL merge is faster than pandas concatenation. but if you go for this route beware of the type of join. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework. \n",
    "\n",
    "\n",
    "The objective of this exercise is to build a class that helps us find missing data in high frequency time series. \n",
    "\n",
    "#### Build a class called DataHandler that initiates with a csv file path as argument\n",
    "```\n",
    "def DataHandler:\n",
    "\n",
    "    def __init__(file_path):\n",
    "        ...\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "#### Create a method that aggregates the data every 5 min and every 1 min for every trading day\n",
    "\n",
    "The method should take in count all the time elapsed between the first trade observation and the last observation\n",
    "even when the time bars are not present in the time series. Be careful of not including non-trading time. i.e the time series shouldnt include any time before 930 am or after 16:00 NY Time. \n",
    "\n",
    "for example, if the time series has the bellow data in 2010-01-04\n",
    "\n",
    "```\n",
    "2010-01-04 14:31:00+00:00    135.77\n",
    "2010-01-04 14:33:00+00:00    135.32\n",
    "\n",
    "```\n",
    "\n",
    "it should return\n",
    "```\n",
    "2010-01-04 14:31:00+00:00    135.77\n",
    "2010-01-04 14:32:00+00:00    Nan\n",
    "2010-01-04 14:33:00+00:00    135.32\n",
    "\n",
    "```\n",
    "\n",
    "#### create a method that counts the  average number of NaNs per day in your time serie\n",
    "\n",
    "\n",
    "#### create a method that  visualizes the \"holes\" the time serie by aggregation time\n",
    "\n",
    "\n",
    "**NOTE: The methods should only use pandas or numpy methods. You can use for loops for debugging but final solution shoul only include pandas methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataHandler:\n",
    "\n",
    "\n",
    "    def __init__(self,file_path):\n",
    "        \"\"\"\n",
    "        reads a csv file and stores it as a pandas data frame\n",
    "        :param file_path:\n",
    "        \"\"\"\n",
    "\n",
    "      \n",
    "\n",
    "    def resample_to_frequnecy(self,time_frequency=\"5min\"):\n",
    "        \"\"\"\n",
    "\n",
    "        :param time_frequency:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "     \n",
    "        return \n",
    "\n",
    "\n",
    "    def count_nans(self,time_frequency):\n",
    "        \"\"\"\n",
    "\n",
    "        :param time_frequency:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "     \n",
    "        return \n",
    "    \n",
    "    def visualize_nans(self,time_frequency):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "p38",
   "language": "python",
   "name": "p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
